\documentclass[a4paper,twoside=false,abstract=false,numbers=noenddot,
titlepage=false,headings=small,parskip=half,version=last]{scrartcl}
\usepackage{../lib/header}
\usepackage{../lib/probability}
\begin{document}
\generateheader{Assignment 2}

\begin{exercise}{A1} Compute $\prob{D|T\in{\text{Polytree}}}$ with Bernoulli
    CPD's \\
    Show how to compute $\prob{D|T}$ where $T$ is a GDM which is a polytree and
    $D = \{x_1,...,x_N\}$ (and $x_i$ is an assignment of values to all
    variables of $T$).
    Assume that all variables are binary and all CPD's Bernoulli.
\end{exercise}
\begin{solution}

\end{solution}

\begin{exercise}{A2} Marginalize over non-observed variables \\
    Assume instead that each $x_i$ is an assignment to a subset of the
    variables say $O$.
    Show how to marginalize over $V\setminus O$ (i.e., the non-observed
    variables).
\end{exercise}
\begin{solution}

\end{solution}

\begin{exercise}{11.3} EM for the mixtures of Bernoullis\\ 
    \begin{itemize}
        \item Show that the M step for ML estimation of a mixture of Bernoullis
            is given by
            \begin{equation}
                \mu_{kj} = 
                \frac
                {
                    \sum_i{
                        r_{ik}x_{ij}
                    }
                }
                {
                \sum_i{
                    r_{ik}
                }
            } 
            \end{equation}
        \item Show that the M step for MAP estimation of a mixture of
        Bernoullis with a $\beta(\alpha,\beta)$ prior is given by
            \begin{equation}
                \mu_{kj} = 
                \frac
                {
                    (\sum_i{
                        r_{ik}x_{ij}
                    }) + \alpha - 1
                }
                {
                    (\sum_i{
                        r_{ik}
                    }) + \alpha + \beta - 1
                }
            \end{equation}
    \end{itemize}
\end{exercise}
\begin{solution}
    The M step is to optimize the auxiliary function $Q$ with respect to
    $\pi,\theta'$. $Q$ is the expected log-likelihood with respect to the former
    parameter $\theta$ and the observed data $D$. 
    \footnote{It can be shown that $Q$ is so that the new parameters is always
    better or as good as the last one, but exclude this proof.}
    The expression for this is
    \begin{equation}
        Q(\theta', \theta) = \Expected\left[\ell(\theta')|D,\theta\right]
    \end{equation}
    and to derive the expression we will use when optimizing with ML/MAP we start
    with introducing the latent variable $z_i$ which corresponds to the
    hidden or missing
    variables which basically is the $r.v.$ for how $x_i$
    is in the class $k$ \footnote{Responsibility $r_{ik}\eqdef
    \cprob{z_i=k}{x_i,\theta}$}.
    
    \begin{equation}
       Q(\theta', \theta) = 
       \Expected\left[\sum\limits_i\log \cprob{x_i,z_i}{\theta'}\right] 
    \end{equation}
    since $\Expected$ is linear and we can factor on the different classes and
    the factors becomes given only the parameters $(\pi_k,\theta'_k)$ of class $k$:
    \begin{equation}
       =\sum\limits_i\Expected \left[ 
            \log\left[
                \prod\limits_k{(
                    \pi_k\cprob{x_i}{\theta'_k}
                )^{\II(z_i=k)}} 
            \right]
        \right] 
    \end{equation}
    then log the inner parts and let $\Expected$ operate on the expression like
    this:
    \begin{equation}
        =\sum\limits_i\sum\limits_k\Expected
            \left[
                \II(z_i=k)
            \right]
            \log\left[
                \cprob{x_i}{\theta'_k}
            \right]
    \end{equation}
    then we put the expected $\Expected\left[\II(z_i=k)\right]$ to be
    $\cprob{z_i=k}{x_i,\theta}=r_{ik}$ that is the expected class belonging of
    $x_i$ given the previous parameters $\theta$.
    Now put in the denotation $r_{ik}$ and use the log on product rule to
    finally get
    \begin{equation}
        = \sum\limits_i \sum\limits_k {r_{ik}\log \pi_k} +
        \sum\limits_i \sum\limits_k r_{ik} \log \cprob{x_i}{\theta'_k}
    \end{equation}
    
    Now since we have that $\sum\limits_{i,k}r_{ik}\log\pi_k \perp
    \sum\limits_{i,k}r_{ik}\log\cprob{x_i}{\theta'_k}$ we can optimize the parameters
    $\pi_k$ and $\theta'_k$ separately and only the $\theta'_k$-term is asked for in the
    exercise which will be denoted by $\ell(\theta'_k)$.

    The model parameters in this case is denoted by $\mu_k$ which is a vector
    and in index-notation $\mu_{kj}$\footnote{Not using Einstein notation for
    Tensor product.}.

    We start with the ML
    \begin{equation}
        \hat{\mu}'_k = \argmax{\mu'_k}{\left\{\sum\limits_i
        r_{ik}\log\cprob{x_i}{\mu'_k}\right\}}
    \end{equation}
    which we find by $\partdev{\ell(\mu'_k)}{\mu'_k}=0$, where 
    $\prod\limits_j \cprob{x_{ij}}{\mu'_{kj}}$ 
    is the multivariate Bernoulli for class $k$
    \begin{eqnarray}
        \partdev{
            \left(
            \ell(\mu'_k)
            \right)
        }
        {
            \mu'_{kj}
        } = 
        \partdev{
            \sum\limits_i r_{ik}\log \prod\limits_{j'} \cprob{x_{ij'}}{\mu'_{kj'}}
        }
        {
            \mu'_{kj} 
        }=
        \partdev{
        \sum\limits_{i,j'} r_{ik}\log\cprob{x_{ij'}}{\mu'_{kj'}} 
        }
        {
            \mu'_{kj}
        }= \left\{
            \partdev{
                \sum\limits_{j\neq j'} (\cdot)
            }
            {
                \mu'_{kj}
            } = 0
            \right\}  = \\
        =
        \partdev{
            \sum\limits_{i}\log \left(
                {\mu'_{kj}}^{x_{ij}}
                (1-{\mu'_{kj}})^{(1-x_{ij})}
            \right)
        }
        {
            \mu'_{kj}
        }
        =
        \partdev{
            \sum\limits_{i} x_{ij}\log{\mu'_{kj}} + (1-x_{ij})\log({1-\mu'_{kj}})
        }
        {
            \mu'_k  
        } = \\
        = \sum\limits_{i} \frac{x_{ij}}{\mu'_{kj}} -
        \frac{1-x_{ij}}{1-\mu'_{kj}} =
        \sum\limits_i \frac{
            \mu'_{kj} - x_{ij}
        }
        {
            \mu'_{kj}(\mu'_{kj}-1)    
        }
    \end{eqnarray}
    now find the zero of this expression
    \begin{equation}
         \sum\limits_i \frac{
            \mu'_{kj} - x_{ij}
        }
        {
            \mu'_{kj}(\mu'_{kj}-1)    
        } = 0
    \end{equation}
    \begin{equation}
         \sum\limits_i
            \mu'_{kj} - x_{ij}
         = 0
    \end{equation}
    \begin{equation}
       \#\left\{x_i\right\}\mu'_{kj} = \sum\limits_i x_{ij} 
    \end{equation}

\end{solution}

%-----------------------
\end{document}
