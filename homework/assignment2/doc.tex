\documentclass[a4paper,twoside=false,abstract=false,numbers=noenddot,
titlepage=false,headings=small,parskip=half,version=last]{scrartcl}
\usepackage{../lib/header}
\usepackage{../lib/probability}
\begin{document}
\generateheader{Assignment 2}

\begin{exercise}{A1} Compute $\prob{D|T\in{\text{Polytree}}}$ with Bernoulli
    CPD's \\
    Show how to compute $\prob{D|T}$ where $T$ is a DGM which is a polytree and
    $D = \{x_1,...,x_N\}$ (and $x_i$ is an assignment of values to all
    variables of $T$).
    Assume that all variables are binary and all CPD's Bernoulli.
\end{exercise}
\begin{solution}

\end{solution}

\begin{exercise}{A2} Marginalize over non-observed variables \\
    Assume instead that each $x_i$ is an assignment to a subset of the
    variables say $O$.
    Show how to marginalize over $V\setminus O$ (i.e., the non-observed
    variables).
\end{exercise}
\begin{solution}

\end{solution}

\begin{exercise}{11.3} EM for the mixtures of Bernoullis\\ 
    \begin{itemize}
        \item Show that the M step for ML estimation of a mixture of Bernoullis
            is given by
            \begin{equation}
                \label{eq:EM_ML_Ber}
                \mu_{kj} = 
                \frac
                {
                    \sum_i{
                        r_{ik}x_{ij}
                    }
                }
                {
                \sum_i{
                    r_{ik}
                }
            } 
            \end{equation}
        \item Show that the M step for MAP estimation of a mixture of
        Bernoullis with a $\beta(\alpha,\beta)$ prior is given by
            \begin{equation}
                \label{eq:EM_MAP_Ber}
                \mu_{kj} = 
                \frac
                {
                    (\sum_i{
                        r_{ik}x_{ij}
                    }) + \alpha - 1
                }
                {
                    (\sum_i{
                        r_{ik}
                    }) + \alpha + \beta - 1
                }
            \end{equation}
    \end{itemize}
\end{exercise}
\begin{solution}
    The M step is to optimize the auxiliary function $Q$ with respect to
    $\pi,\theta'$. $Q$ is the expected posterior log-likelihood with respect 
    to the last  parameter $\theta$ and the observed data $D$. 
    \footnote{It can be shown that $Q$ is so that the new parameters is always
    better or as good as the last one, but exclude this proof.}
    The expression for this is
    \begin{equation}
        Q(\theta', \theta) = \Expected\left[\ell(\theta')|D,\theta\right]
    \end{equation}
    and to derive this expression 
    we introducing the latent variable $z_i$ which corresponds to the
    hidden or missing
    variables which basically is the $r.v.$ for how $x_i$
    belongs to the class $k$ \footnote{Responsibility $r_{ik}\eqdef
    \cprob{z_i=k}{x_i,\theta}$}.
    We start with MAP and then derive ML by setting a uniform priori.
    \begin{equation}
       Q(\theta', \theta) = 
       \Expected\left[\sum\limits_i\log
       \cprob{x_i,z_i}{\theta'}\prob{\theta'}\right] =
    \end{equation}
    since $\Expected$ is linear and we can factor on the different classes and
    the factors become given with the parameters $(\pi_k,\theta'_k)$ of class $k$:
    \begin{equation}
       =\sum\limits_i\Expected \left[ 
            \log\left[
                \prod\limits_k{(
                    \pi_k\cprob{x_i}{\theta'_k}
                )^{\II(z_i=k)}}
                \prob{\theta'_k}
            \right]
        \right] = 
    \end{equation}
    then log the inner parts and let $\Expected$ operate on the expression 
    \begin{equation}
        =\sum\limits_i\sum\limits_k\Expected
            \left[
                \II(z_i=k)
            \right]
            \log\left[
                \cprob{x_i}{\theta'_k}
            \right] + \log\prob{\theta'_k} = 
    \end{equation}
    then we have that the expected $\Expected\left[\II(z_i=k)\right]$ will be
    $\cprob{z_i=k}{x_i,\theta}=r_{ik}$ which is the expected class-belonging of
    $x_i$ given the previous parameters $\theta$.
    The log product rule gives us
    \begin{equation}
        = \sum\limits_i \sum\limits_k {r_{ik}\log \pi_k} +
        \sum\limits_i \sum\limits_k \left\{ r_{ik} \log \cprob{x_i}{\theta'_k} +
        \log \prob{\theta'_k}
        \right\}
    \end{equation}
    
    Now since we have that $\sum\limits_{i,k}r_{ik}\log\pi_k \perp
    \sum\limits_{i,k}\left\{r_{ik}\log\cprob{x_i}{\theta'_k}+\log
    \prob{\theta'_k}\right\}$ we can optimize the parameters
    $\pi_k$ and $\theta'_k$ separately and only the $\theta'_k$-term is asked for in the
    exercise which we denote $\ell(\theta'_k)$.

    The model parameters in this case is denoted by $\mu_k$ which is a vector
    and in index-notation $\mu_{kj}$
    \footnote{Not using Einstein notation for Tensor product.}.

    We start with the MAP
    \begin{equation}
        \hat{\mu}'_k = \argmax{\mu'_k}{\left\{\sum\limits_i
        r_{ik}\log\cprob{x_i}{\mu'_k}+\log\prob{\mu'_k}\right\}}
    \end{equation}
    which we find by $\partdev{\ell(\mu'_k)}{\mu'_k}=0$, where 
    $\cprob{x_i}{\mu'_k}=\prod_j \cprob{x_{ij}}{\mu'_{kj}}$ 
    is the multivariate Bernoulli\footnote{Often called multinoulli.}
    distribution for class $k$. The priori in the same way is
    $\prob{\mu'_k}=\prod_j{\prob{\mu'_{kj}}}$ and we will keep it as a general
    distribution for as long as possible to make it easier to do ML later but
    in the MAP case is $\beta(\alpha,\beta)
    =\frac{{\mu'_{kj}}^{\alpha-1}(1-\mu'_{kj})^{\beta-1}}{B(\alpha,\beta)}$.

    \begin{eqnarray}
        \label{eq:MAP_general}
        \partdev{
            \left(
            \ell(\mu'_k)
            \right)
        }
        {
            \mu'_{kj}
        } = 
        \partdev{
            \sum\limits_i r_{ik}\log \prod\limits_{j'} \cprob{x_{ij'}}{\mu'_{kj'}}
            + \log\prod\limits_{j'} \prob{\mu'_{kj'}}
        }
        {
            \mu'_{kj} 
        }=
        \partdev{
            \sum\limits_{i,j'} r_{ik}\log\cprob{x_{ij'}}{\mu'_{kj'}}
            + \log \prob{\mu'_{kj'}}
        }
        {
            \mu'_{kj}
        }=\\=
        \left\{
            \partdev{
                \sum\limits_{j\neq j'} (\cdot)
            }
            {
                \mu'_{kj}
            } = 0
        \right\} = 
        \partdev{
            \sum\limits_{i}r_{ik}\log \left(
                {\mu'_{kj}}^{x_{ij}}
                (1-{\mu'_{kj}})^{(1-x_{ij})}
            \right)
            + \log \frac{
                    {\mu'_{kj}}^{\alpha-1} (1-\mu'_{kj})^{\beta-1}
                }{
                    B(\alpha,\beta)
                }
        }
        {
            \mu'_{kj}
        }
        = \\ =
        \partdev{
            \sum\limits_{i}r_{ik} x_{ij}\log{\mu'_{kj}} +
            r_{ik}(1-x_{ij})\log({1-\mu'_{kj}})+
            (\alpha-1)\log\mu'_{kj}+(\beta-1)\log(1-\mu'_{kj})-\log B(\alpha,\beta)
        }
        {
            \mu'_k  
        }  
        = \\ = 
        \sum\limits_{i} \frac{r_{ik}x_{ij}+\alpha-1}{\mu'_{kj}} -
        \frac{r_{ik}(1-x_{ij})+\beta-1}{1-\mu'_{kj}} 
        = CONTINUE HERE 
        \sum\limits_i r_{ik}\frac{
            \mu'_{kj} - x_{ij}
        }
        {
            \mu'_{kj}(\mu'_{kj}-1)    
        }
    \end{eqnarray}
    now find the zero of this expression
    \begin{equation}
         \sum\limits_i r_{ik}\frac{
            \mu'_{kj} - x_{ij}
        }
        {
            \mu'_{kj}(\mu'_{kj}-1)    
        } = 0
    \end{equation}
    \begin{equation}
         \sum\limits_i
            r_{ik}\mu'_{kj} - r_{ik}x_{ij}
         = 0
    \end{equation}
    \begin{equation}
        \mu'_{kj}\sum\limits_i r_{ik} = \sum\limits_i r_{ik} x_{ij} 
    \end{equation}

    \begin{equation}
        \mu'_{kj} = \frac{
            \sum_i r_{ik}x_{ij}
        }
        {
            \sum_i r_{ik}
        }
    \end{equation}

    which is the same as the equation \eqref{eq:EM_ML_Ber}.
    

\end{solution}

%-----------------------
\end{document}
