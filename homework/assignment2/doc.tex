\documentclass[a4paper,twoside=false,abstract=false,numbers=noenddot,
titlepage=false,headings=small,parskip=half,version=last]{scrartcl}
\usepackage{../lib/header}
\usepackage{../lib/probability}
\begin{document}
\generateheader{Assignment 2}

\begin{exercise}{A1} Compute $\prob{D|T\in{\text{Polytree}}}$ with Bernoulli
    CPD's \\
    Show how to compute $\prob{D|T}$ where $T$ is a GDM which is a polytree and
    $D = \{x_1,...,x_N\}$ (and $x_i$ is an assignment of values to all
    variables of $T$).
    Assume that all variables are binary and all CPD's Bernoulli.
\end{exercise}
\begin{solution}

\end{solution}

\begin{exercise}{A2} Marginalize over non-observed variables \\
    Assume instead that each $x_i$ is an assignment to a subset of the
    variables say $O$.
    Show how to marginalize over $V\setminus O$ (i.e., the non-observed
    variables).
\end{exercise}
\begin{solution}

\end{solution}

\begin{exercise}{11.3} EM for the mixtures of Bernoullis\\ 
    \begin{itemize}
        \item Show that the M step for ML estimation of a mixture of Bernoullis
            is given by
            \begin{equation}
                \mu_{kj} = 
                \frac
                {
                    \sum_i{
                        r_{ik}x_{ij}
                    }
                }
                {
                \sum_i{
                    r_{ik}
                }
            } 
            \end{equation}
        \item Show that the M step for MAP estimation of a mixture of
        Bernoullis with a $\beta(\alpha,\beta)$ prior is given by
            \begin{equation}
                \mu_{kj} = 
                \frac
                {
                    (\sum_i{
                        r_{ik}x_{ij}
                    }) + \alpha - 1
                }
                {
                    (\sum_i{
                        r_{ik}
                    }) + \alpha + \beta - 1
                }
            \end{equation}
    \end{itemize}
\end{exercise}
\begin{solution}
    The M step is to optimize the auxiliary function $Q$ with respect to
    $\pi,\theta'$. $Q$ is the expected log-likelihood with respect to the former
    parameter $\theta$ and the observed data $D$. 
    \footnote{It can be shown that $Q$ is so that the new parameters is always
    better or as good as the last one, but exclude this proof.}
    The expression for this is
    \begin{equation}
        Q(\theta', \theta) = \Expected\left[\ell(\theta')|D,\theta\right]
    \end{equation}
    and to derive the expression we will use when optimizing with ML/MAP we start
    with introducing the latent variable $z_i$ which corresponds to the
    hidden or missing
    variables which basically is the $r.v.$ for how $x_i$
    is in the class $k$ \footnote{Responsibility $r_{ik}\eqdef
    \cprob{z_i=k}{x_i,\theta}$}.
    
    \begin{equation}
       Q(\theta', \theta) = 
       \Expected\left[\sum\limits_i\log \cprob{x_i,z_i}{\theta'}\right] 
    \end{equation}
    since $\Expected$ is linear and we can 
    \begin{equation}
       =\sum\limits_i\Expected \left[ 
            \log\left[
                \prod\limits_k{(
                    \pi_k\cprob{x_i}{\theta_k}
                )^{\II(z_i=k)}} 
            \right]
        \right] 
    \end{equation}



\end{solution}

%-----------------------
\end{document}
