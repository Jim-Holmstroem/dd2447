\documentclass[a4paper,twoside=false,abstract=false,numbers=noenddot,
titlepage=false,headings=small,parskip=half,version=last]{scrartcl}
\usepackage{../lib/header}
\usepackage{../lib/probability}
\begin{document}
\generateheader{Assignment 2}

\begin{exercise}{A1} Compute $\prob{D|T\in{\text{Polytree}}}$ with Bernoulli
    CPD's \\
    Show how to compute $\prob{D|T}$ where $T$ is a DGM which is a polytree and
    $D = \{x_1,...,x_N\}$ (and $x_i$ is an assignment of values to all
    variables of $T$).
    Assume that all variables are binary and all CPD's Bernoulli.
\end{exercise}
\begin{solution}

\end{solution}

\begin{exercise}{A2} Marginalize over non-observed variables \\
    Assume instead that each $x_i$ is an assignment to a subset of the
    variables say $O$.
    Show how to marginalize over $V\setminus O$ (i.e., the non-observed
    variables).
\end{exercise}
\begin{solution}

\end{solution}

\begin{exercise}{11.3} EM for the mixtures of Bernoullis\\ 
    \begin{itemize}
        \item Show that the M step for ML estimation of a mixture of Bernoullis
            is given by
            \begin{equation}
                \label{eq:EM_ML_Ber}
                \mu_{kj} = 
                \frac
                {
                    \sum_i{
                        r_{ik}x_{ij}
                    }
                }
                {
                \sum_i{
                    r_{ik}
                }
            } 
            \end{equation}
        \item Show that the M step for MAP estimation of a mixture of
        Bernoullis with a $\beta(\alpha,\beta)$ prior is given by
            \begin{equation}
                \label{eq:EM_MAP_Ber}
                \mu_{kj} = 
                \frac
                {
                    (\sum_i{
                        r_{ik}x_{ij}
                    }) + \alpha - 1
                }
                {
                    (\sum_i{
                        r_{ik}
                    }) + \alpha + \beta - 2
                }
            \end{equation}
    \end{itemize}
\end{exercise}
\begin{solution}
    The M step is to optimize the auxiliary function $Q$ with respect to
    $\pi,\theta'$. $Q$ is the expected posterior log-likelihood\footnote{In the
    book it's actually just log-likelihood, but this will work in the same way 
    instead of using $Q(\theta',\theta)+\log\prob{\theta'}$ without derivation,
    we will use posterior-Q as Q instead.} with respect 
    to the last  parameter $\theta$ and the observed data $D$. 
    \footnote{It can be shown that $Q$ is so that the new parameters is always
    better or as good as the last one, but exclude the proof for this since
    it's not needed by the exercise.}
    The expression for this is
    \begin{eqnarray}
        Q(\theta', \theta) = 
        \Expected\left[\log\LL_{MAP}(\theta')|D,\theta\right] =
        \Expected\left[\log\left(\LL(\theta')\prob{\theta'}\right)|D,\theta\right]=\\=
        \Expected\left[\ell(\theta') + \log\prob{\theta'}|D,\theta\right]=
        \Expected\left[\ell(\theta') |D,\theta\right] + \log\prob{\theta'}
    \end{eqnarray}
    and to derive this expression 
    we introducing the latent variable $z_i$ which corresponds to the
    hidden or missing
    variables which basically is the $r.v.$ for how $x_i$
    belongs to the class $k$. \footnote{Responsibility $r_{ik}\eqdef
    \cprob{z_i=k}{x_i,\theta}$}
    We start with MAP and then derive ML by setting a uniform priori.
    \begin{equation}
       Q(\theta', \theta) = 
       \Expected\left[\sum\limits_i\log
       \cprob{x_i,z_i}{\theta'}\right] + \prob{\theta'} =
    \end{equation}
    since $\Expected$ is linear and we can factor on the different classes and
    the factors become given with the parameters $(\pi_k,\theta'_k)$ of class $k$:
    \begin{equation}
       =\sum\limits_i\left[ \Expected 
            \log\left(
                \prod\limits_k{(
                    \pi_k\cprob{x_i}{\theta'_k}
                )^{\II(z_i=k)}}
            \right)
        \right] + \log\prod\limits_k\prob{\theta'_k} = 
    \end{equation}
    then log the inner parts and let $\Expected$ operate on the expression 
    \begin{equation}
        =\sum\limits_i\sum\limits_k\bigg[ \Expected
            \left[
                \II(z_i=k)
            \right]
            \log\left[
                \cprob{x_i}{\theta'_k}
            \right]\bigg] + \sum\limits_k\log\prob{\theta'_k} = 
    \end{equation}
    then we have that the expected $\Expected\left[\II(z_i=k)\right]$ will be
    $\cprob{z_i=k}{x_i,\theta}=r_{ik}$ which is the expected class-belonging of
    $x_i$ given the previous parameters $\theta$.
    The log product rule gives us
    \begin{equation}
        = \sum\limits_i \sum\limits_k {r_{ik}\log \pi_k} +
        \sum\limits_i \sum\limits_k \left\{ r_{ik} \log \cprob{x_i}{\theta'_k}
        \right\} + 
        \sum\limits_k\log\prob{\theta'_k}
    \end{equation}
    
    Now since we have that 
    $\sum\limits_{i,k}r_{ik}\log\pi_k 
    \perp
    \sum\limits_{i,k}
        \left\{
            r_{ik}\log\cprob{x_i}{\theta'_k}
        \right\}+
        \sum\limits_k\log\prob{\theta'_k}$ 
    we can optimize the parameters
    $\pi_k$ and $\theta'_k$ separately and only the $\theta'_k$-term is asked for in the
    exercise which we denote $\ell(\theta'_k)$.

    The model parameters in this case is denoted by $\mu_k$ which is a vector
    and in index-notation $\mu_{kj}$.
    \footnote{Not using Einstein notation for tensor product.}

    We start with the MAP
    \begin{equation}
        \hat{\mu}'_k = 
        \argmax
        {
            \mu'_k
        }{
            \left\{
                \sum\limits_i
                    \left\{
                        r_{ik}\log\cprob{x_i}{\mu'_k}
                    \right\}
                +\log\prob{\mu'_k}
            \right\}
        }
    \end{equation}
    which we find by $\partdev{\ell(\mu'_k)}{\mu'_k}=0$, where 
    $\cprob{x_i}{\mu'_k}=\prod_j \cprob{x_{ij}}{\mu'_{kj}}$ 
    is the multivariate Bernoulli\footnote{Often called multinoulli.}
    distribution for class $k$. The priori in the same way is
    $\prob{\mu'_k}=\prod_j{\prob{\mu'_{kj}}}$.
    In our case for MAP this is $\beta(\alpha,\beta)
    =\frac{{\mu'_{kj}}^{\alpha-1}(1-\mu'_{kj})^{\beta-1}}{B(\alpha,\beta)}$.

    \begin{eqnarray}
        \label{eq:MAP_general}
        \partdev{
            \left(
                \ell(\mu'_k)
            \right)
        }
        {
            \mu'_{kj}
        } = 
        \partdev{
            \sum\limits_i\left\{
                r_{ik}\log \prod\limits_{j'} 
                \cprob{x_{ij'}}{\mu'_{kj'}}\right\}
            + \log\prod\limits_{j'} \prob{\mu'_{kj'}}
        }
        {
            \mu'_{kj} 
        }=\\=
        \partdev{
            \sum\limits_{i,j'}\left\{r_{ik}\log\cprob{x_{ij'}}{\mu'_{kj'}}\right\}
            + \sum\limits_{j'}\log\prob{\mu'_{kj'}}
        }
        {
            \mu'_{kj}
        }=\\=
        \left\{
            \partdev{
                \sum\limits_{j\neq j'} (\cdot)
            }
            {
                \mu'_{kj}
            } = 0
        \right\} = 
        \partdev{
            \sum\limits_i 
            \left\{
                r_{ik}\log 
                \left(
                    {\mu'_{kj}}^{x_{ij}}
                    (1-{\mu'_{kj}})^{(1-x_{ij})}
                \right)
            \right\}
            + \log\frac{
                    {\mu'_{kj}}^{\alpha-1} (1-\mu'_{kj})^{\beta-1}
                }{
                    B(\alpha,\beta)
                }
        }
        {
            \mu'_{kj}
        }
        = \\ = 
        \partdev{
            \sum\limits_i r_{ik}\left(x_{ij}\log{\mu'_{kj}} +
            (1-x_{ij})\log({1-\mu'_{kj}})\right)
        }
        {
            \mu'_k  
        }
        + \\ + 
        \partdev{
            (\alpha-1)\log\mu'_{kj}+(\beta-1)\log(1-\mu'_{kj})-\log B
        }
        {
            \mu'_k  
        }
        = \\ = 
        \frac{\left(\sum_i r_{ik}   x_{ij} \right)+\alpha-1}{  \mu'_{kj}} -
        \frac{\left(\sum_i r_{ik}(1-x_{ij})\right)+ \beta-1}{1-\mu'_{kj}} 
        = \\ =
        \frac{
            (1-\mu'_{kj})\left(\sum_i r_{ik}x_{ij}\right)-
            \mu'_{kj}\left(\sum_i r_{ik}(1-x_{ij})\right)
        }
        {
            \mu'_{kj}(1-\mu'_{kj}) 
        } + \\ +
        \frac
        {
            (1-\mu'_{kj})\left(\alpha-1\right)-
            \mu'_{kj}\left(\beta-1\right)
        }
        {
            \mu'_{kj}(1-\mu'_{kj}) 
        } = \\ =
        \frac
        {
            \left(\sum_i r_{ik}x_{ij}\right) - 
            \mu'_{kj}\left(\sum_i r_{ik}\right) +
            (\alpha-1)-\mu'_{kj}(\alpha+\beta-2)
        }
        {
            \mu'_{kj}(1-\mu'_{kj}) 
        }
    \end{eqnarray}
    and now find the zero of this expression
    \begin{equation}
       \frac
        {
            \left(\sum_i r_{ik}x_{ij}\right) - 
            \mu'_{kj}\left(\sum_i r_{ik}\right) +
            (\alpha-1)-\mu'_{kj}(\alpha+\beta-2)
        }
        {
            \mu'_{kj}(1-\mu'_{kj}) 
        } = 0
    \end{equation}
    \begin{equation}
        \left(\textstyle\sum_i r_{ik}x_{ij}\right) - 
        \mu'_{kj}\left(\textstyle\sum_i r_{ik}\right) +
        (\alpha-1)-\mu'_{kj}(\alpha+\beta-2)
         = 0
    \end{equation}
    \begin{equation}
        \mu'_{kj} \left[\left(\textstyle\sum_i r_{ik}\right)+\alpha+\beta-2\right] 
        = \left(\textstyle\sum_i r_{ik} x_{ij}\right)+\alpha-1
    \end{equation}
    \begin{equation}
        \mu'_{kj} = \frac{
            \left(\sum_i r_{ik} x_{ij}\right)+\alpha-1
        }
        {
            \left(\sum_i r_{ik}\right)+\alpha+\beta-2
        }
    \end{equation}

    which is the same as the equation \eqref{eq:EM_MAP_Ber}.\qed
   
    By just looking at the expression for $\beta(\cdot)$ we see that
    \begin{equation}
        \beta(1,1) = U(0,1) 
    \end{equation}
    and knowing that ML is a MAP with a uniform priori we can just set
    $\alpha,\beta=1$ in the derived expression to get the ML.
    \begin{equation}
        \mu'_{kj} =
        \frac{\sum_ir_{ik}x_{ij}+1-1}{\sum_ir_{ik}+1+1-2} =
        \frac{\sum_ir_{ik}x_{ij}}{\sum_ir_{ik}}
    \end{equation}
    which is the same as the equation \eqref{eq:EM_ML_Ber}.\qed

\end{solution}

%-----------------------
\end{document}
