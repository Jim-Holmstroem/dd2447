\documentclass[a4paper,twoside=false,abstract=false,numbers=noenddot,
titlepage=false,headings=small,parskip=half,version=last]{scrartcl}
\usepackage{../lib/header}
\usepackage{../lib/probability}
\begin{document}
\generateheader{Assignment 3}
In all cases you are supposed to provide as sound algorithms of the types
described in the course.
Summing over exponentially many terms and other
solutions that always gives exponential time, will not be considered
sufficient.
\begin{exercise}{2} MAP switch setting and position on model railway \\ 
    Imagine a model railway with a single train. You know the map of the tracks
    including the position of all the switches, but you don't know current states
    of the switches, or where the train is currently located. Each switch has three
    connections: $\left\{0,L,R\right\}$. If the train comes from the direction of
    $L$ or $R$, it always
    leaves in the direction $0$. If the train comes from the direction $0$, it will
    leave in either direction $L$ or $R$, depending on the state of the switch. The
    switch has prior probability $1/2$ for each direction, but will remain the same
    throughout the train run. You are receiving a stream of signals from the train,
    each signal specifying the direction in which train has passed a switch: 
    $\left\{0L,0R,L0,R0\right\}$; you do not know, however, 
    which switch the train has passed.
    Also, the sensors are noisy, and with a certain probability $p$, the train
    reports a random signal instead of a real direction in which it passed the
    switch.

    Given the map of the railroad and a sequence of switch signals
    $\left\{ y_i \right\}_{i=1}^T$. Your task is to compute MAP switch and current
    position.
\end{exercise}
\begin{solution}
    Represent the connections on switch $\alpha$ with 
    $\left\{\alpha_0,\alpha_L,\alpha_R\right\}$. 

    The outline for this solution is to reduce the original problem to one
    which fulfills the Markov assumption and can thus be used without being a
    approximation in a ordinary HMM.

    ``Current position'' can either be the position of the last passed switch
    which is hinted by the assignment having a list of positions for the
    switches given or it could be defined as being in between two connections.
    If we have the first case we simply pick the switch from the last of the
    two connections we are between according to MAP solution. 

    Let $\cdot\leftrightarrow\cdot$ denote a link between connections in the
    given map.

    Construct a switch-graph
    \footnote{Not a real graph yet because of the
    switches, they could be deduced at this step but it will just make it
    harder.}
    $G$ of the map where each switch is represented by a triangle with the
    side $LR$ in bold,
    \footnote{Arbitrary representation of a switch but it catches that it is
    one object and that you cannot pass between $L$ and $R$.} 
    each connection is a node, and the edges is given by the map information
    $\cdot\leftrightarrow\cdot$. 

    A simple example of a map would be the two switches $\alpha,\beta$ with the
    links 
    $\left\{
        \alpha_L\leftrightarrow\beta_L,
        \alpha_0\leftrightarrow\beta_R,
        \alpha_R\leftrightarrow\beta_0
    \right\}$

    If we were to use this representation of states we stumble into
    problems when you are in state $\alpha_0$ since you don't know if you came
    from $\alpha_L$ or $\alpha_R$ and thus cannot decide the correct 
    distribution for the observation. A possible solution to this would be to
    perform a directed lifting of the graph $G$ and denote this by $G'$.
    In the simple example we will have the \underline{nodes}
    $\left\{
        \alpha_L\rightarrow\beta_L,
        \alpha_L\leftarrow\beta_L,
        \alpha_0\rightarrow\beta_R,
        \alpha_0\leftarrow\beta_R,
        \alpha_R\rightarrow\beta_0,
        \alpha_R\leftarrow\beta_0
    \right\}$.
    But 


    %Note about sparsity in graph and lifted graph

\end{solution}

\begin{exercise}{3}Gibbs sampler for posterior of magic word generative model\\
    The following generative model generates $K$ sequences of length $N$:
    $\left\{s_i\right\}_{i=1}^K$ where $s_i=\left\{s_{i,j}\right\}_{j=1}^N$.
    All sequences are over the alphabet $\left[M\right]$. Each of these
    sequences has a ``magic'' word of length $w$ hidden in it and the rest of
    the sequence is called background.

    First $\forall i$, a start position $r_i$ for the magic word is sampled
    uniformly from $\left[N-w+1\right]$. Then the $j$:th positions in the words
    are sampled from $q_j(x)$, which is $Cat\left(x|\theta_j\right)$ where 
    $\theta_j$ has a $Dir\left(\theta_j|\alpha\right)$ prior. All other
    positions in the sequences are sampled from the background distribution
    $q(x)$, which is $Cat\left(x|\theta\right)$ where $\theta$ has a
    $Dir\left(\theta|\alpha'\right)$ prior.
\end{exercise}
\begin{solution}
    
\end{solution}
%-----------------------
\end{document}
