\documentclass[a4paper,twoside=false,abstract=false,numbers=noenddot,
titlepage=false,headings=small,parskip=half,version=last]{scrartcl}
\usepackage{../lib/header}
\usepackage{../lib/probability}
\begin{document}
\generateheader{Assignment 3}
In all cases you are supposed to provide as sound algorithms of the types
described in the course.
Summing over exponentially many terms and other
solutions that always gives exponential time, will not be considered
sufficient.
\begin{exercise}{2} MAP switch setting and position on model railway \\ 
    Imagine a model railway with a single train. You know the map of the tracks
    including the position of all the switches, but you don't know current states
    of the switches, or where the train is currently located. Each switch has three
    connections: $\left\{0,L,R\right\}$. If the train comes from the direction of
    $L$ or $R$, it always
    leaves in the direction $0$. If the train comes from the direction $0$, it will
    leave in either direction $L$ or $R$, depending on the state of the switch. The
    switch has prior probability $1/2$ for each direction, but will remain the same
    throughout the train run. You are receiving a stream of signals from the train,
    each signal specifying the direction in which train has passed a switch: 
    $\left\{0L,0R,L0,R0\right\}$; you do not know, however, 
    which switch the train has passed.
    Also, the sensors are noisy, and with a certain probability $p$, the train
    reports a random signal instead of a real direction in which it passed the
    switch.

    Given the map of the railroad and a sequence of switch signals
    $\left\{ y_i \right\}_{i=1}^T$. Your task is to compute MAP switch and current
    position.
\end{exercise}
\begin{solution}
    Represent the connections on switch $\alpha$ with 
    $\left\{\alpha_0,\alpha_L,\alpha_R\right\}$. 

    The outline for this solution is to reduce the original problem to one
    which fulfills the Markov assumption and can thus be used without being a
    approximation in a ordinary HMM.

    ``Current position'' can either be the position of the last passed switch
    which is hinted by the assignment having a list of positions for the
    switches given or it could be defined as being in between two connections.
    If we have the first case we simply pick the switch from the last of the
    two connections we are between according to MAP solution. 

    Let $\cdot\leftrightarrow\cdot$ denote a undirected link
    \footnote{Use the word ``link'' instead of ``connection'' to avoid 
    name collision}
    between connections in the given map.

    Construct a switch-graph
    \footnote{Not a real graph yet because of the
    switches, they could be deduced at this step but it will just make it
    harder.}
    $G$ of the map where each switch is represented by a triangle with the
    side $LR$ in bold,
    \footnote{Arbitrary representation of a switch but it catches that it is
    one object and that you cannot pass between $L$ and $R$.} 
    each connection is a node, and the edges is given by the map information
    $\cdot\leftrightarrow\cdot$. 

    A simple example of a map would be the two switches $\alpha,\beta$ with the
    links 
    $\left\{
        \alpha_L\leftrightarrow\beta_L,
        \alpha_0\leftrightarrow\beta_R,
        \alpha_R\leftrightarrow\beta_0
    \right\}$

    If we were to use this representation of states we stumble into
    problems when you are in state $\alpha_0$ since you don't know if you came
    from $\alpha_L$ or $\alpha_R$ and thus cannot decide the correct 
    distribution for the observation. A possible solution
    \footnote{Another possible solution is to lift it on the links and then
    lift that graph again, which was my first try. I think that also worked but
    it became a real mess.}
    to this would be to
    perform a directed lifting inside the switches of the graph $G$ and the
    links becomes undirected edges between the new states, denote 
    this by $G'$.
    So $\forall \alpha \in \text{switches}$ we have the $4$ states:
    %$\left\{
    %    \alpha_0\rightarrow\alpha_L,
    %    \alpha_0\leftarrow\alpha_L,
    %    \alpha_0\rightarrow\alpha_R,
    %    \alpha_0\leftarrow\alpha_R
    %\right\}$.
    Which gives us a total of $4T$ states for $G'$.

    Note that $G'$ is sparse with only $O(T)$ edges both in the
    undirected inherited from $G$ as well as the lifted directed ones.

    Each link from the original map constructs a constraint on the transition
    matrix $A$. And can be listed as follows; the sparsity renders most
    transitions $0$ along with the diagonal and thus we will only list the
    non-zero ones.
    Looking locally at a connection we have either it being a in- or output and 
    the same goes for the connected neighbour to it. In this case in- and
    output basically means input$=\left\{\alpha_0\right\}$ and
    output$=\left\{\alpha_L,\alpha_R\right\}$.

    For output linked to output we have:
    \begin{eqnarray}
        \alpha_L\leftrightarrow\beta_L \Rightarrow 
        (\alpha_0\leftarrow\alpha_L) 
            &\rightarrow&
        (\beta_0\rightarrow\beta_L) 
        = 1,\nonumber \\
        (\beta_0\leftarrow\beta_L) 
            &\rightarrow&
        (\alpha_0\rightarrow\alpha_L) 
        = 1 \nonumber \\
    \end{eqnarray}
    
    For input to output (and output to input by symmetry) we have:
    \begin{eqnarray}
        \alpha_0\leftrightarrow\beta_L \Rightarrow 
        (\alpha_0\leftarrow\alpha_L)
            &\rightarrow&
        (\beta_0\leftarrow\beta_L) = 1, \nonumber \\
        (\alpha_0\leftarrow\alpha_R)
            &\rightarrow&
        (\beta_0\leftarrow\beta_L) = 1, \nonumber \\
        (\beta_0\leftarrow\beta_L)
            &\rightarrow&
        (\alpha_0\rightarrow\alpha_L) = P_\alpha, \nonumber \\
        (\beta_0\leftarrow\beta_L)
            &\rightarrow&
        (\alpha_0\rightarrow\alpha_R) = 1-P_\alpha \nonumber \\
    \end{eqnarray}

    For input to input we have:
    \begin{eqnarray}
        \alpha_0\leftrightarrow\beta_L \Rightarrow 
        (\alpha_0\leftarrow\alpha_L)
            &\rightarrow&
        (\beta_0\leftarrow\beta_L) = 1, \nonumber \\
        (\alpha_0\leftarrow\alpha_R)
            &\rightarrow&
        (\beta_0\leftarrow\beta_L) = 1, \nonumber \\
        (\beta_0\leftarrow\beta_L)
            &\rightarrow&
        (\alpha_0\rightarrow\alpha_L) = P_\alpha, \nonumber \\
        (\beta_0\leftarrow\beta_L)
            &\rightarrow&
        (\alpha_0\rightarrow\alpha_R) = 1-P_\alpha \nonumber \\
    \end{eqnarray}

    and from this all possible local cases comes from symmetry.




    Using the lecture slide 9 
    ``learning transition and emission parameters''
    but in our case we have constraints on the transition matrix and the
    observation matrix is already given.
    Since finding the exact transition table is intractable in general we ..

\end{solution}

\begin{exercise}{3}Gibbs sampler for posterior of magic word generative model\\
    The following generative model generates $K$ sequences of length $N$:
    $\left\{s_i\right\}_{i=1}^K$ where $s_i=\left\{s_{i,j}\right\}_{j=1}^N$.
    All sequences are over the alphabet $\left[M\right]$. Each of these
    sequences has a ``magic'' word of length $w$ hidden in it and the rest of
    the sequence is called background.

    First $\forall i$, a start position $r_i$ for the magic word is sampled
    uniformly from $\left[N-w+1\right]$. Then the $j$:th positions in the words
    are sampled from $q_j(x)$, which is $Cat\left(x|\theta_j\right)$ where 
    $\theta_j$ has a $Dir\left(\theta_j|\alpha\right)$ prior. All other
    positions in the sequences are sampled from the background distribution
    $q(x)$, which is $Cat\left(x|\theta\right)$ where $\theta$ has a
    $Dir\left(\theta|\alpha'\right)$ prior.
\end{exercise}
\begin{solution}
    
\end{solution}
%-----------------------
\end{document}
    \end{equation}

    Using the lecture slide 9 
    ``learning transition and emission parameters''
    but in our case we have constraints on the transition matrix and the
    observation matrix is already given.

\end{solution}

\begin{exercise}{3}Gibbs sampler for posterior of magic word generative model\\
    The following generative model generates $K$ sequences of length $N$:
    $\left\{s_i\right\}_{i=1}^K$ where $s_i=\left\{s_{i,j}\right\}_{j=1}^N$.
    All sequences are over the alphabet $\left[M\right]$. Each of these
    sequences has a ``magic'' word of length $w$ hidden in it and the rest of
    the sequence is called background.

    First $\forall i$, a start position $r_i$ for the magic word is sampled
    uniformly from $\left[N-w+1\right]$. Then the $j$:th positions in the words
    are sampled from $q_j(x)$, which is $Cat\left(x|\theta_j\right)$ where 
    $\theta_j$ has a $Dir\left(\theta_j|\alpha\right)$ prior. All other
    positions in the sequences are sampled from the background distribution
    $q(x)$, which is $Cat\left(x|\theta\right)$ where $\theta$ has a
    $Dir\left(\theta|\alpha'\right)$ prior.
\end{exercise}
\begin{solution}
    
\end{solution}
%-----------------------
\end{document}
