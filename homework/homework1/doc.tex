\documentclass[a4paper,twoside=false,abstract=false,numbers=noenddot,
titlepage=false,headings=small,parskip=half,version=last]{scrartcl}
\usepackage{header}
\usepackage{probability}
\begin{document}
\generateheader{Homework 1}

\begin{exercise}{2.16} Mean, mode, variance for the beta distribution \\
    Suppose $\theta \sim Beta(a,b)$. Derive the mean, mode and variance.
\end{exercise}
\begin{solution}
    Denote $Beta(a,b)=Beta_{a,b}$ to simplify the expressions later on. The
    mean of a random variable sampled from $Beta(a,b)$ is defined as:
    \begin{equation}
        \Expected\theta = \int_\Omega\theta'Beta_{a,b}(\theta')d\theta'
    \end{equation}
    where 
    \begin{equation}
        Beta_{a,b}(\theta)=\frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)},
        B(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}
    \end{equation}
    and $\Omega$ is the support $[0,1]$
    \begin{lemma}\label{lem:Bplus1}
        \begin{equation}
            B(a+1,b)=B(b,a) \bigwedge B(a+1,b)=\frac{a}{a+b}B(a,b)
        \end{equation}
    \end{lemma}
    \begin{proof}
        The first relation is given by the symmetry in the definition of $B$
        together with a commutative multiplication.
        For the second relation we first need to show the well known identity
        \begin{equation}
            \Gamma(z+1)=z\Gamma(z)
        \end{equation}
        By the definition of $\Gamma(z) \eqdef
        \int\limits_0^\infty{u^{z-1}e^{-u}du}$
        and by using partial integration we get
        \begin{equation}
            \Gamma(z+1)=\int{u^ze^{-u}du}=\big[-u^ze^{-u}\big]_{u=0}^\infty+
            \int\limits_0^\infty{zu^{z-1}e^{-u}du}=z\int\limits_0^\infty{u^{z-1}e^{-u}du}=z\Gamma(z)
        \end{equation}
        Having this relation we can show that
        \begin{equation}
            B(a+1,b)=\frac{\Gamma(a+1)\Gamma(b)}{\Gamma(a+1+b)}=
            \frac{a\Gamma(a)\Gamma(b)}{(a+b)\Gamma(a+b)}=\frac{a}{a+b}B(a,b)
        \end{equation}
    \end{proof}
    
    Starting out by deriving the mean: 
    \begin{equation}
        \begin{split}
            \Expected\theta = \int\theta'Beta_{a,b}(\theta')d\theta' =
            \int\frac{B(a+1,b)Beta_{a+1,b}(\theta')}{B(a,b)}d\theta' = \\
            = \frac{B(a+1,b)}{B(a,b)}\int{Beta_{a+1,b}(\theta')d\theta'} =
            \frac{B(a+1,b)}{B(a,b)} = \{\text{Using Lemma \ref{lem:Bplus1}}\}=\\
            =\frac{aB(a,b)}{(a+b)B(a,b)}=\frac{a}{a+b}
        \end{split}
    \end{equation}
    
    Next we derive the mode which is the most occurring $\theta$
    \begin{equation}
        Mode(\theta)=\argmax{\theta'}(Beta_{a,b}(\theta'))
    \end{equation}
    To find it we use
    \begin{equation}
        \partdev{Beta_{a,b}(\theta)}{\theta}=0,
        \doublepartdev{Beta_{a,b}(\theta)}{\theta}<0
    \end{equation}

    \begin{equation}
        \begin{split}
            0=B(a,b)\partdev{Beta_{a,b}(\theta)}{\theta}=\{\text{prod. rule}\}= \\
            =(a-1)\theta^{a-2}(1-\theta)^{b-1}-(b-1)\theta^{a-1}(1-\theta)^{b-2}
        \end{split}
    \end{equation}
    which gives us the equation
    \begin{equation}
        (a-1)\theta^{a-2}(1-\theta)^{b-1}=(b-1)\theta^{a-1}(1-\theta)^{b-2}
    \end{equation}
    \begin{equation}
        (a-1)(1-\theta)=(b-1)\theta 
    \end{equation}
    \begin{equation}
        \theta=\frac{1}{1+\frac{b-1}{a-1}}=\frac{a-1}{a-1+b-1}=\frac{a-1}{a+b-2} 
    \end{equation}
    Finally we deal with the variance: 
    \begin{equation}
        \begin{split}
            \Variance(\theta)=\Expected\theta^2 - (\Expected\theta)^2=
            \int\theta'^2 Beta_{a,b}(\theta')d\theta' - (\Expected\theta)^2= \\
            = \frac{B(a+2,b)}{B(a,b)} - (\Expected\theta)^2 = \{\text{Using
            Lemma \ref{lem:Bplus1}}\}=
            \frac{(a+1)}{(a+b+1)}\frac{a}{(a+b)} - \bigg(\frac{a}{a+b}\bigg)^2 =
            \\
            = \frac{a}{a+b}\bigg(\frac{(a+1)(a+b)-a(a+b+1)}{(a+b)(a+b+1)}\bigg)
            = \frac{ab}{(a+b)^2(a+b+1)}
        \end{split}    
    \end{equation}

\end{solution}

\begin{exercise}{3.6} MLE for the Poisson distribution \\
    The Poisson pmf is defined as
    $Poi(x|\lambda)=\Poi{\lambda}{x}$ for $x \in \{0,1,2,..\}$
    where $\lambda>0$ is the rate parameter. Derive the MLE. 
\end{exercise}
\begin{solution}
    The data $D = \{x_i\}$ is $i.i.d \bigwedge x_i\sim Poi(\lambda)$
    \begin{equation}
        \cprob{x_i}{\lambda} = \Poi{\lambda}{x_i}
    \end{equation}
    The probability for $D$:
    \begin{equation}
        L(\lambda)=\cprob{\{x_i\}}{\lambda}=\prod\limits_i{\cprob{x_i}{\lambda}}=\prod\limits_i \Poi{\lambda}{x_i}
    \end{equation}
    The log-likelihood for the parameter $\lambda$
    \begin{equation}
        \ell(\lambda)=\log(\prod\limits_i\Poi{\lambda}{x_i})=
        \sum\limits_i{(-\lambda+x_i \log\lambda-\log x_i)}
    \end{equation}
    Find the maximum likely $\lambda$ by finding the maximum $\log$-likely
    $\lambda$ which will coincide since $\log$ is a strictly increasing
    function.
    \begin{equation}
        \partdev{\ell(\lambda)}{\lambda} = 0
    \end{equation}
    \begin{equation}
        \partdev{\ell(\lambda)}{\lambda} =
        \sum\limits_i{\bigg(-1+\frac{x_i}{\lambda}\bigg)}=-|\{x_i\}|+\frac{\sum{x_i}}{\lambda}=0
    \end{equation}
    Which gives MLE for $\lambda$:
    \begin{equation}
        \lambda=\frac{\sum{x_i}}{|\{x_i\}|}(=\Expected x)
    \end{equation}
\end{solution}

\begin{exercise}{3.7} Bayesian analysis of the Poisson distribution \\
    In exercise 3.6, we defined the Poisson distribution with rate $\lambda$
    and derived its MLE. Here we perform a conjugate Bayesian analysis. 
    \begin{description}
        \item[a.] Derive the posterior $p(\lambda|D)$ assuming a conjugate
            prior $p(\lambda)=Ga(\lambda|a,b)\propto
            \lambda^{a-1}e^{-\lambda b}$. Hint: the posterior is also a Gamma
            distribution. 
        \item[b.] What does the posterior mean tend to as $a\rightarrow 0$ and
            $b\rightarrow 0$? (Recall that the mean of a $Ga(a,b)$ distribution
            is $a/b$.)
    \end{description}
\end{exercise}
\begin{solution}
    $Ga(a,b)$ is denoted $\Gamma(a,b)$ and $a,b>0$ of a $\Gamma$-distribution.
    \begin{equation}
        \begin{split}
            \cprob{\lambda}{D}\propto
            \cprob{D}{\lambda}p(\lambda)=\lambda^{a-1}e^{-\lambda
            b}\prod\limits_i{(\Poi{\lambda}{x_i})}\propto
            \lambda^{a-1}e^{-\lambda
            b}e^{-|D|\lambda}\prod\limits_i{\lambda^{x_i}}= \\
            =e^{-|D|\lambda}e^{-\lambda}\lambda^{a-1}\lambda^{\sum{x_i}}=
            e^{-(|D|+b)\lambda}\lambda^{\sum{x_i}+a-1}\propto
            \Gamma\bigg(\sum{x_i}+a,|D|+b\bigg)
        \end{split}
    \end{equation}
    with the parameters for the $\Gamma$-function being $a'=\sum x_i + a \ge a > 0$
    and $b' = |D| + b \ge b > 0$ together with the fact that
    $\cprob{\lambda}{D}$ is a distribution, that is properly normalized, it will
    result in not only being proportional but equally distributed:
    \begin{equation}
        \cprob{\lambda}{D}=\Gamma\bigg(\sum{x_i}+a,|D|+b\bigg)
    \end{equation}
    still acting as a distribution, the samples
    \begin{equation}
        X \sim \Gamma\bigg(\sum{x_i}+a,|D|+b\bigg)
    \end{equation}
    in the limit the $\Expected$ becomes
    \begin{equation}
        \lim_{a,b\rightarrow 0} \Expected X = \lim_{a,b\rightarrow 0}\frac{\sum
        x_i+a}{|D|+b}=\frac{\sum x_i}{|D|} (=E x)
    \end{equation}
    
\end{solution}

%-----------------------
\end{document}
